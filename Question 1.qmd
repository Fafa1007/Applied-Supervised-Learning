# Question 1 - Modelling

```{r}
train <- read.csv("online_shopping_train.csv")
test <- read.csv("online_shopping_testing.csv")
valid <- read.csv("online_shopping_valid.csv")
head(train)
```

First convert categorical variables into factor type, stating for which features this was done.

```{r}
factor_cols <- c("Administrative", "Informational", "ProductRelated", "SpecialDay", "Month", "OperatingSystems", "Browser", "VisitorType", "Weekend")

scale_cols <- c("Administrative_Duration", "Informational_Duration", "ProductRelated_Duration","BounceRates", "ExitRates", "PageValues")

train[factor_cols] <- lapply(train[factor_cols], factor)
train[scale_cols] <- lapply(train[scale_cols], scale)
y_train <- train["Revenue"]
x_train <- train[, -16]

valid[factor_cols] <- lapply(valid[factor_cols], factor)
valid[scale_cols] <- lapply(valid[scale_cols], scale)
y_valid <- valid["Revenue"]
x_valid <- valid[,-16]

test[head(factor_cols,-1)] <- lapply(test[head(factor_cols,-1)], factor)
test[scale_cols] <- lapply(test[scale_cols], scale)
```

------------------------------------------------------------------------

# Logistic regression with a linear decision boundary.

Apply elastic-net regularization to this model, motivating for the choice of α and λ.

```{r}
library(tidyverse)
library(knitr)
library(broom)

# logistic regression model to the entire datase
log_mod1 <- glm(Revenue ~ ., family = binomial,data = train)
log_mod1 %>% 
  tidy() %>%
  kable(digits = 2, caption = 'Saturated logistic regression model fitted to the online shopping dataset')
```

```{r}
library(glmnet)
library(plotmo)

# Fit the lasso (L1 regularisation) and plot using plotmo
train_l1 <- glmnet(x_train, as.matrix(y_train), alpha = 1, standardize = T, family = 'binomial')
plot_glmnet(train_l1, xvar = 'norm')
```

```{r}
library(glmnet)
library(plotmo)

# Fit the ridge (L2 regularisation) and plot using plotmo
train_l2 <- glmnet(x_train, as.matrix(y_train), alpha = 0,standardize = T, family = 'binomial')
plot_glmnet(train_l2, xvar = 'lambda')

# Determine the appropriate lamda level using CV with MSE as the loss function
train_l2_cv <- cv.glmnet(as.matrix(x_valid), as.numeric(y_valid[[1]]), alpha = 0, nfolds = 10, type.measure = 'mse', standardize = T)
```
