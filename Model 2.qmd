# Logistic Regression With A Non-Linear Decision Boundary

Apply 10-fold cross-validation throughout to measure classification accuracy

## Modelling

You may specify the model and regularize (or not) in any way you wish, just be sure to clearly state your final model.

Vanilla Model

```{r}
# Vanilla Logistic Regression Model
log_mod <- glm(Revenue ~ ., family = binomial,data = train)
log_mod %>% 
  tidy() %>%
  kable(digits = 2, caption = 'Vanilla Logistic Regression Model')
```

Polynomial Logistic Regression Models (choose the variables to make 5th degree polynomials by looking at the most significant numerical variables from the vanilla model)

1.  Polynomial Logistic Model with all variables

```{r}
poly_log1 <- glm(Revenue ~ 
            Informational + I(Informational^2) + I(Informational^3) +I(Informational^4) + 
            ProductRelated_Duration + I(ProductRelated_Duration^2) + I(ProductRelated_Duration^3) + I(ProductRelated_Duration^4) +
            ExitRates + I(ExitRates^2) + I(ExitRates^3) + I(ExitRates^4) + 
            PageValues + I(PageValues^2) + I(PageValues^3) + I(PageValues^4) + .
            ,data = train, family = 'binomial')
poly_log1 %>% 
  tidy() %>%
  kable(digits = 2, caption = 'Polynomial logistic regression model fitted to the online shopping dataset using all the varibles')
```

2.  Polynomial Logistic Model with only the polynomial variables

```{r}
poly_log2 <- glm(Revenue ~ 
            Informational + I(Informational^2) + I(Informational^3) +I(Informational^4) + 
            ProductRelated_Duration + I(ProductRelated_Duration^2) + I(ProductRelated_Duration^3) + I(ProductRelated_Duration^4) +
            ExitRates + I(ExitRates^2) + I(ExitRates^3) + I(ExitRates^4) + 
            PageValues + I(PageValues^2) + I(PageValues^3) + I(PageValues^4) 
            ,data = train, family = 'binomial')
poly_log2 %>% 
  tidy() %>%
  kable(digits = 2, caption = 'Polynomial logistic regression model fitted to the online shopping dataset using only the polynomial variables')

```

------------------------------------------------------------------------

\newpage

## Inference/Interpretation

1.  **Strongest Predictors**:

    -   `PageValues` has the [largest positive effect]{.underline} on the target variable, with a coefficient of 3.31 (p \< 0.001), indicating that a one-unit increase in PageValues increases the log-odds of the target variable by 3.31.

    -   Conversely, `ExitRates` has the [strongest negative effect]{.underline}, with a coefficient of -0.68 (p = 0.01), meaning a one-unit increase in ExitRates decreases the log-odds by -0.68. These variables are highly significant and play a critical role in influencing outcomes.

2.  **Nonlinear Effects**:

    -   Higher-order terms for `PageValues` reveal a complex nonlinear relationship: quadratic term = **-1.07** (p \< 0.001), cubic term = **0.12** (p \< 0.001), and quartic term = **0.00** (p \< 0.001). All are highly significant.

3.  **Seasonal Effects**:

    -   Several months show significant effects on the target variable. For example, **February** has a strong negative effect with a coefficient of **-1.71** (p = 0.03), while **November** has a positive effect with a coefficient of **0.51** (p = 0.01). Other significant months include **March** (coefficient = **-0.72**, p \< 0.001), **May** (coefficient = **-0.77**, p \< 0.001), and **December** (coefficient = **-0.74**, p \< 0.001). These results suggest strong seasonal trends in the data.

4.  **Insignificant Predictors**:

    -   Many predictors are not statistically significant (p â‰¥ 0.05), meaning they do not have a strong effect on the target variable. These include:

        -   `Informational` and its higher-order terms (p-values: 0.11 to 0.28).

        -   `ProductRelated_Duration` and its higher-order terms (p-values: 0.08 to 0.71).

        -   `Administrative_Duration` (p = 0.92), `Informational_Duration` (p = 0.80), and `SpecialDay` (p = 0.90).

        -   Months like **July** (p = 0.50), **June** (p = 0.32), **October** (p = 0.46), and **September** (p = 0.87).

        -   Operating systems (`OperatingSystems2`: p = 0.75; `OperatingSystems3`: p = 0.33; `OperatingSystems4`: p = 0.09).

        -   Browsers (`Browser2`: p = 0.84; `Browser3`: p = 0.45; `Browser4`: p = 0.65; `Browser6`: p = 0.34; `Browser10`: p = 0.65).

        -   Visitor types like `VisitorTypeOther` (p = 0.34).

```{r}
# Coefficients
poly_log1 %>% 
  tidy() %>%
  kable(digits = 2, caption = 'Polynomial logistic regression model fitted to the online shopping dataset using all the varibles')
```

------------------------------------------------------------------------

\newpage

## Model Evaluation

```{r}
library(MLmetrics)
library(caret)
library(knitr)
library(dplyr)
library(ROCR)

# Firstly, get predicted probabilities or predicted classe (gives the same conclusions) from the model
pred_probabilities <- predict(poly_log1,x_valid, type = "response")

# Secondly, create a confusion matrix
yhat <- ifelse(pred_probabilities >= 0.5,'Revenue', "No Revenue")
y <- matrix(ifelse(as.numeric(unlist(y_valid))==2, 'Revenue', "No Revenue"))
cat("Confusion Matrix using Predicted Probabilities")
(confmat <- table(yhat, y, dnn = c('Predicted label', 'True label')))

# Thirdly, get the Accuracy, F1 Score, Precision, Recall, Specificity which we can calculate from the confusion matrix .... or us the caret package 
caretmat <- confusionMatrix(as.factor(yhat), as.factor(y), positive = 'Revenue')
caretmat 

Sensitivity <- caretmat$byClass["Sensitivity"]  # True Positive Rate (Recall)
Specificity <- caretmat$byClass["Specificity"]  # True Negative Rate
Precision <- caretmat$byClass["Precision"]      # Positive Predictive Value
F1_score <- caretmat$byClass["F1"]              # F1 Score
Accuracy <- caretmat$overall["Accuracy"]        # Accuracy
Missclassification_Rate <- mean(yhat != y)

# AUC (only works with binary data)
yhat <- ifelse(pred_probabilities >= 0.5, 1, 0)
y <- matrix(ifelse(as.numeric(unlist(y_valid))==2, 1, 0))
pred_obj <- prediction(y, yhat)
ROC_AUC <- performance(pred_obj, measure = "auc")@y.values[[1]]

# Lastly, put them all into a table
combined <- cbind(Sensitivity, Specificity, Precision, F1_score,Accuracy, Missclassification_Rate, ROC_AUC)
rownames(combined) <- c("Polynomial Logistic Regression")
kable(combined)

compar <- rbind(compar, combined)
```

------------------------------------------------------------------------

\newpage

## Optimizing F1 Score

------------------------------------------------------------------------

\newpage

## Prediction
