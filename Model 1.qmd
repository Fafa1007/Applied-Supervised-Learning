# Logistic Regression With A Linear Decision Boundary (Elastic Regularisation).

Apply 10-fold cross-validation throughout to measure classification accuracy

## Modelling

Vanilla Logistic Regression Model

```{r}
library(tidyverse)
library(knitr)
library(broom)

# Vanilla logistic regression model to the entire dataset
log_mod <- glm(Revenue ~ ., family = binomial,data = train)
log_mod %>% 
  tidy() %>%
  kable(digits = 2, caption = 'Vanilla logistic regression model')
```

Finding and motivating our choice of α and λ hyper parameters for the Elastic-Net Regularisation.

```{r}
# Elastic Net
library(glmnet)
library(plotmo)
library(glmnetUtils)
set.seed(12)

# Firstly, prepare the data for elastic-net regularization by creating dummy variables for factor columns
train_dummies <- makeX(x_train)

# Secondly, perform 10 fold cross-validation to find the optimal alpha and lambda hyperparameters
elasticnet_cv <- cva.glmnet(train_dummies, as.numeric(unlist(y_train)), alpha = seq(0, 1, 0.1), nfolds = 10)

# Thirdly, plot the cross-validation error against lambda for different fixed values of alpha
plot(elasticnet_cv, main ="Cross Validation Error as lambda increases for 
different fixed values of alpha")

# Fourthly, extract the alpha and lambda hyperparameters that give the lowest cross-validation error
alphas <- elasticnet_cv$alpha 
cv_mses <- sapply(elasticnet_cv$modlist,
  function(mod) min(mod$cvm) 
  )

lowest_mse <- round(min(cv_mses),5)
best_alpha <- alphas[which.min(cv_mses)]
best_lambda <- round(elasticnet_cv$modlist[[which.min(cv_mses)]]$lambda.min,6)

print(paste("The lowest CV MSE is", lowest_mse))
print(paste("The Alpha corresponding to this minumum is", best_alpha))
print(paste("The Lamda acorresponding to this minumum is", best_lambda))

# Lastly, plot the lowest MSE for each alpha and highlight the best alpha 
plot(alphas, cv_mses, 'b', lwd = 2, pch = 16, col = 'navy', xlab = expression(alpha), ylab = 'CV MSE')
abline(v = best_alpha, lty = 3, col = 'red')
```

Finally our Logistic Regression Model with Elastic-Net Regularization

```{r}
library(glmnet)

# Fit a logistic regression model with elastic-net regularization using the best alpha and lambda
log_elas <- glmnet(train_dummies, as.numeric(unlist(y_train)), family = "binomial", alpha = best_alpha, lambda = best_lambda, standardize = T)
```

------------------------------------------------------------------------

\newpage

## Inference/Interpretation

-   **Strongest Predictors**:

    -   PageValues has the largest positive effect on the target variable, with a coefficient of 1.442, meaning a one-unit increase in PageValues increases the log-odds of the Revenue by 1.442.
    -   ExitRates has the strongest negative effect, with a coefficient of -0.752, meaning a one-unit increase in ExitRates decreases the log-odds of Revenue by -0.752.
    -   These variables play a critical role in influencing outcomes.

-   **Regularization Impact**:

    -   Several variables (e.g., SpecialDay, MonthJune, OperatingSystems1, Browser1, Browser2, VisitorTypeReturning_Visitor) were excluded from the model due to regularization, highlighting the importance of feature selection in reducing overfitting.

-   **Temporal and Behavioral Insights**:

    -   Months like November and July show positive effects, while February has a strong negative effect, suggesting seasonal trends. Visitor types like New_Visitor also positively influence outcomes, whereas Other visitor types have a negative effect.

-   **Negligible or Weak Effects**:

    -   The following variables have minimal or negligible effects, indicating they contribute little to predicting the target variable: WeekendTRUE, Administrative, Administrative_Duration, Informational, ProductRelated, ProductRelated_Duration, BounceRates, MonthAug, MonthDec, MonthMar, MonthMay, MonthOct, MonthSep, OperatingSystems2, OperatingSystems3, OperatingSystems4, Browser3, Browser4, Browser5, Browser6, Browser8, Browser10, WeekendFALSE

```{r}
# Coefficients
coef(log_elas)
```

------------------------------------------------------------------------

\newpage

## Model Evaluation decision threshold of 0.5

```{r}
library(MLmetrics)
library(caret)
library(knitr)
library(dplyr)
library(ROCR)

# Firstly, get predicted probabilities or predicted classe (gives the same conclusions) from model
x_valid_dummies <- makeX(x_valid)
pred_probabilities <- predict(log_elas, newx = as.matrix(x_valid_dummies), type = "response")

# Secondly, create a confusion matrix
yhat <- ifelse(pred_probabilities >= 0.5, 'Revenue', 'No Revenue')
y <- matrix(ifelse(as.numeric(unlist(y_valid))==2, 'Revenue', "No Revenue"))
cat("Confusion Matrix using Predicted Probabilities")
(confmat <- table(yhat, y, dnn = c('Predicted label', 'True label')))

# Thirdly, get the Accuracy, F1 Score, Precision, Recall, Specificity which we can calculate from the confusion matrix .... or us the caret package 
caretmat <- confusionMatrix(as.factor(yhat), as.factor(y), positive = 'Revenue')
caretmat 

Sensitivity <- caretmat$byClass["Sensitivity"]  # True Positive Rate (Recall)
Specificity <- caretmat$byClass["Specificity"]  # True Negative Rate
Precision <- caretmat$byClass["Precision"]      # Positive Predictive Value
F1_score <- caretmat$byClass["F1"]              # F1 Score
Accuracy <- caretmat$overall["Accuracy"]        # Accuracy
Missclassification_Rate <- mean(yhat != y)

# AUC need the values to be binary
yhat <- ifelse(pred_probabilities >= 0.5, 1, 0)
y <- matrix(ifelse(as.numeric(unlist(y_valid))==2, 1, 0))
pred_obj <- prediction(y, yhat)
ROC_AUC <- performance(pred_obj, measure = "auc")@y.values[[1]]

# Lastly, put them all into a table
combined <- cbind(Sensitivity, Specificity, Precision, F1_score,Accuracy, Missclassification_Rate, ROC_AUC)
rownames(combined) <- c("Logistic Regression with Elastic Regularisation")
kable(combined)

compar <- rbind(compar,combined)
```

------------------------------------------------------------------------

\newpage

## Optimizing F1 Score

```{r}
auc_cv <- cv.glmnet(x_train_dummies, y_train, family = 'binomial', type.measure = 'auc', keep = T)
all_rocs <- roc.glmnet(auc_cv$fit.preval, newy = y_train) #67 different curves!

best_roc <- auc_cv$index['min', ] #Also labeled min, even though here it's a max :|

round(coef(auc_cv, s = 'lambda.min'), 3)
```

------------------------------------------------------------------------

\newpage

## Prediction
