# Logistic Regression With A Linear Decision Boundary (Elastic Regularisation).

## Modelling

Vanilla Logistic Regression Model

```{r}
library(tidyverse)
library(knitr)
library(broom)

# Vanilla logistic regression model to the entire dataset
log_mod <- glm(Revenue ~ ., family = binomial,data = train)
log_mod %>% 
  tidy() %>%
  kable(digits = 2, caption = 'Vanilla logistic regression model')
```

Finding and motivating our choice of α and λ hyper parameters for the Elastic-Net Regularisation.

```{r}
# Elastic Net
library(glmnet)
library(plotmo)
library(glmnetUtils)
set.seed(12)

# Firstly, prepare the data for elastic-net regularization by creating dummy variables for factor columns
train_dummies <- makeX(x_train)

# Secondly, perform cross-validation to find the optimal alpha and lambda hyperparameters
elasticnet_cv <- cva.glmnet(train_dummies, as.numeric(unlist(y_train)), alpha = seq(0, 1, 0.1), nfolds = 10)

# Thirdly, plot the cross-validation error against lambda for different fixed values of alpha
plot(elasticnet_cv, main ="Cross Validation Error as lambda increases for 
different fixed values of alpha")

# Fourthly, extract the alpha and lambda hyperparameters that give the lowest cross-validation error
alphas <- elasticnet_cv$alpha 
cv_mses <- sapply(elasticnet_cv$modlist,
  function(mod) min(mod$cvm) 
  )

lowest_mse <- round(min(cv_mses),5)
best_alpha <- alphas[which.min(cv_mses)]
best_lambda <- round(elasticnet_cv$modlist[[which.min(cv_mses)]]$lambda.min,6)

print(paste("The lowest CV MSE is", lowest_mse))
print(paste("The Alpha corresponding to this minumum is", best_alpha))
print(paste("The Lamda acorresponding to this minumum is", best_lambda))

# Lastly, plot the lowest MSE for each alpha and highlight the best alpha 
plot(alphas, cv_mses, 'b', lwd = 2, pch = 16, col = 'navy', xlab = expression(alpha), ylab = 'CV MSE')
abline(v = best_alpha, lty = 3, col = 'red')
```

Finally our Logistic Regression Model with Elastic-Net Regularization

```{r}
library(glmnet)

# Fit a logistic regression model with elastic-net regularization using the best alpha and lambda
log_elas <- glmnet(train_dummies, as.numeric(unlist(y_train)), family = "binomial", alpha = best_alpha, lambda = best_lambda, standardize = T)

log_elas %>% 
  tidy() %>%
  kable(digits = 2, caption = paste('Logistic Regression Model With Elastic Regularisation and lambda is ',best_lambda, "and alpha", best_alpha))
```

------------------------------------------------------------------------

\newpage

## Inference/Interpretation

------------------------------------------------------------------------

\newpage

## Model Evaluation

```{r}

```

------------------------------------------------------------------------

\newpage

## Optimizing F1 Score

------------------------------------------------------------------------

\newpage

## Prediction
