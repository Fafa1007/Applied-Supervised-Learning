# K-Nearest Neighbours (KNN)

Apply 10-fold cross-validation throughout to measure classification accuracy

#### Set-Up

```{r}
library(class)
library(ggplot2)
library(caret)
library(gbm)
library(xgboost)
library(ROCR)
library(dplyr)
library(tidyverse)
library(knitr)

train <- read.csv("online_shopping_train.csv")
test <- read.csv("online_shopping_testing.csv")
valid <- read.csv("online_shopping_valid.csv")

head(train)

factor_cols <- c("Month", "OperatingSystems", "Browser", "VisitorType", "Weekend", "Revenue")

scale_cols <- c("Administrative","Administrative_Duration", "Informational", "Informational_Duration","ProductRelated","SpecialDay", "ProductRelated_Duration","BounceRates", "ExitRates", "PageValues")

train[factor_cols] <- lapply(train[factor_cols], factor)
train[scale_cols] <- lapply(train[scale_cols], scale)
y_train <- train["Revenue"]
x_train <- train[, -16]

valid[factor_cols] <- lapply(valid[factor_cols], factor)
valid[scale_cols] <- lapply(valid[scale_cols], scale)
y_valid <- valid["Revenue"]
x_valid <- valid[,-16]

test[head(factor_cols,-1)] <- lapply(test[head(factor_cols,-1)], factor)
test[scale_cols] <- lapply(test[scale_cols], scale)
```

## Modelling

```{r}
#| warning: false

N <- 30

knn_model <- train(Revenue ~ ., 
                data = train,
                method = "knn",
                trControl = trainControl(method = "cv", number = 10),
                tuneGrid = expand.grid(k = 1:N))


## knn_pred <- predict(knn_model, valid[,-16])

save(knn_model, file = 'R Data/gbm.Rdata')
load('R Data/gbm.Rdata')

```

------------------------------------------------------------------------

\newpage

## Model Evaluation (ques 1 b)
```{r}
knn_predict_probabilities <- predict(knn_model, newdata = valid, type = "prob")[, "1"]

# Secondly, create a confusion matrix
yhat <- ifelse(knn_predict_probabilities >= 0.5,'Revenue', "No Revenue")
y <- matrix(ifelse(as.numeric(unlist(valid[,16]))==2, 'Revenue', "No Revenue"))
cat("Confusion Matrix using Predicted Probabilities")
(confmat <- table(yhat, y, dnn = c('Predicted label', 'True label')))

caretmat <- confusionMatrix(as.factor(yhat), as.factor(y), positive = 'Revenue')
caretmat 

Sensitivity <- caretmat$byClass["Sensitivity"]  # True Positive Rate (Recall)
Specificity <- caretmat$byClass["Specificity"]  # True Negative Rate
Precision <- caretmat$byClass["Precision"]      # Positive Predictive Value
F1_score <- caretmat$byClass["F1"]              # F1 Score
Accuracy <- caretmat$overall["Accuracy"]        # Accuracy
Missclassification_Rate <- mean(yhat != y)

# AUC (only works with binary data)
yhat <- ifelse(knn_predict_probabilities >= 0.5, 1, 0)
y <- matrix(ifelse(as.numeric(unlist(valid[,16]))==2, 1, 0))
pred_obj <- prediction(y, yhat)
ROC_AUC <- performance(pred_obj, measure = "auc")@y.values[[1]]

# Lastly, put them all into a table
combined <- cbind(Sensitivity, Specificity, Precision, F1_score,Accuracy, Missclassification_Rate, ROC_AUC)
rownames(combined) <- c("Polynomial Logistic Regression")
kable(combined)

compar[3,] <- combined
```

------------------------------------------------------------------------


\newpage

## Inference/Interpretation

```{r}

```

------------------------------------------------------------------------

\newpage

## Optimising F1 Score

```{r}
#| warning: false
#| message: false

# Finding Maximum F1 Score and Corresponding Tau
knn_predict_probabilities <- predict(knn_model, newdata = valid, type = "prob")[, "1"]

F1_score_vec <- vector()
tau <- vector()
seq_i <- seq(min(knn_predict_probabilities) + 0.0001 ,max(knn_predict_probabilities) - 0.0001,length = 1000)
index <- 1

for (i in seq_i) {
  yhat_knn <- ifelse(knn_predict_probabilities >= i,'Revenue', "No Revenue")
  y_knn <- matrix(ifelse(as.numeric(unlist(valid[,16]))==2, 'Revenue', "No Revenue"))
  confmat <- table(yhat_knn, y_knn, dnn = c('Predicted label', 'True label'))

  caretmat <- confusionMatrix(as.factor(yhat_knn), as.factor(y_knn), positive = 'Revenue')
   
  F1_score_vec[index] <- caretmat$byClass["F1"]
  tau[index] <- i
  index <- index + 1
}

df <- data.frame(x = tau, y = F1_score_vec)
mf_knn <- max(F1_score_vec)
max_F1_tau_knn <- df$x[df$y == mf_knn]

# Plotting F Score Account Tau
df <- data.frame(x = tau, y = F1_score_vec)

f1_t_3 <- ggplot() + geom_line(data = df, aes(x = x, y = y)) + geom_vline(xintercept = mean(max_F1_tau_knn), color = "red", linetype = "dashed") + theme_minimal() + labs(x = "Tau", y = "F1 Score", title = "F1 Score Against Different Desician Rule Thresholds (Tau) for a KNN Model")
f1_t_3

# ROC Curve Plot
pred_knn <- prediction(as.numeric(knn_predict_probabilities), as.numeric(valid[,16]))
perf_knn  <- performance(pred_knn, 'tpr', 'fpr')
plot.new()        # Start a new plot (if needed)
plot(perf_knn, colorize = FALSE, col = 'black')
lines(c(0,1), c(0,1), col = 'gray', lty = 4)

yhat_knn <- ifelse(knn_predict_probabilities >= max_F1_tau_knn,'Revenue', "No Revenue")
y_knn <- matrix(ifelse(as.numeric(unlist(valid[,16]))==2, 'Revenue', "No Revenue"))
cat("Confusion Matrix using Predicted Probabilities")
(confmat <- table(yhat_knn, y_knn, dnn = c('Predicted label', 'True label')))

caretmat_knn <- confusionMatrix(as.factor(yhat_knn), as.factor(y_knn), positive = 'Revenue')

TPR_knn <- caretmat_knn$byClass["Sensitivity"]  # True Positive Rate (Recall)
FPR_knn <- 1 - caretmat_knn$byClass["Specificity"]

points(
  x = FPR_knn, 
  y = TPR_knn,
  pch = 19,          # Solid circle point
  col = "red",       # Color of the point
  cex = 1          # Point size
)

text(
  x = FPR_knn + 0.15,
  y = TPR_knn - 0.16,  # Second label: slightly below the point
  labels = paste("F1 Score:", round(mf_knn, 3)),  # Example metric
  col = "black"
)

text(
  x = FPR_knn + 0.15,
  y = TPR_knn - 0.08,  # Second label: slightly below the point
  labels = paste("Threshold:", round(mean(max_F1_tau_knn), 3)),  # Example metric
  col = "black"
)

# Maximum F1 Score And Corresponding Threshold
mean(max_F1_tau_knn)
mf_knn


```
------------------------------------------------------------------------

\newpage

## Prediction

```{r}

```
