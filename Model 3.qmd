# K-Nearest Neighbours (KNN)

Apply 10-fold cross-validation throughout to measure classification accuracy

## Modelling

```{r}
#| warning: false

library(caret)
library(class) 

set.seed(4046)

# Firstly, define the range of k values for tuning (1 to 30)
N <- 30


# Secondly, perform k-Nearest Neighbors (kNN) with 10-fold cross-validation to find the optimal number of neighbors (k) for classification
knn_model <- train(Revenue ~ ., 
                data = train,
                method = "knn",
                trControl = trainControl(method = "cv", number = 10),
                tuneGrid = expand.grid(k = 1:N))


# Lastly, a plot of the performance of each of the neighbours
plot(knn_model)

save(knn_model, file = 'R Data/K Nearest Neighbours Model.Rdata')

```

------------------------------------------------------------------------

\newpage

## Model Evaluation 

```{r}
library(MLmetrics)
library(caret)
library(knitr)
library(dplyr)
library(ROCR)

# Firstly, get predicted probabilities or predicted classe (gives the same conclusions) from model
knn_predict_probabilities <- predict(knn_cv, newdata = valid, type = "prob")[, "1"]

# Secondly, create a confusion matrix
yhat <- ifelse(knn_predict_probabilities >= 0.5,'Revenue', "No Revenue")
y <- matrix(ifelse(as.numeric(unlist(valid[,16]))==1, 'Revenue', "No Revenue"))
cat("Confusion Matrix using Predicted Probabilities")
(confmat <- table(yhat, y, dnn = c('Predicted label', 'True label')))

caretmat <- confusionMatrix(as.factor(yhat), as.factor(y), positive = 'Revenue')
caretmat 

Sensitivity <- caretmat$byClass["Sensitivity"]  # True Positive Rate (Recall)
Specificity <- caretmat$byClass["Specificity"]  # True Negative Rate
Precision <- caretmat$byClass["Precision"]      # Positive Predictive Value
F1_score <- caretmat$byClass["F1"]              # F1 Score
Accuracy <- caretmat$overall["Accuracy"]        # Accuracy
Missclassification_Rate <- mean(yhat != y)

# AUC (only works with binary data)
yhat <- ifelse(knn_predict_probabilities >= 0.5, 1, 0)
y <- matrix(ifelse(as.numeric(unlist(valid[,16]))==2, 1, 0))
pred_obj <- prediction(y, yhat)
ROC_AUC <- performance(pred_obj, measure = "auc")@y.values[[1]]

# Lastly, put them all into a table
combined <- cbind(Sensitivity, Specificity, Precision, F1_score,Accuracy, Missclassification_Rate, ROC_AUC)
rownames(combined) <- c("Polynomial Logistic Regression")
kable(combined)

compar[3,] <- combined
```

------------------------------------------------------------------------

\newpage

## Model Evaluation (ques 1 b)
```{r}
knn_predict_probabilities <- predict(knn_model, newdata = valid, type = "prob")[, "1"]

# Secondly, create a confusion matrix
yhat <- ifelse(knn_predict_probabilities >= 0.5,'Revenue', "No Revenue")
y <- matrix(ifelse(as.numeric(unlist(valid[,16]))==2, 'Revenue', "No Revenue"))
cat("Confusion Matrix using Predicted Probabilities")
(confmat <- table(yhat, y, dnn = c('Predicted label', 'True label')))

caretmat <- confusionMatrix(as.factor(yhat), as.factor(y), positive = 'Revenue')
caretmat 

Sensitivity <- caretmat$byClass["Sensitivity"]  # True Positive Rate (Recall)
Specificity <- caretmat$byClass["Specificity"]  # True Negative Rate
Precision <- caretmat$byClass["Precision"]      # Positive Predictive Value
F1_score <- caretmat$byClass["F1"]              # F1 Score
Accuracy <- caretmat$overall["Accuracy"]        # Accuracy
Missclassification_Rate <- mean(yhat != y)

# AUC (only works with binary data)
yhat <- ifelse(knn_predict_probabilities >= 0.5, 1, 0)
y <- matrix(ifelse(as.numeric(unlist(valid[,16]))==2, 1, 0))
pred_obj <- prediction(y, yhat)
ROC_AUC <- performance(pred_obj, measure = "auc")@y.values[[1]]

# Lastly, put them all into a table
combined <- cbind(Sensitivity, Specificity, Precision, F1_score,Accuracy, Missclassification_Rate, ROC_AUC)
rownames(combined) <- c("Polynomial Logistic Regression")
kable(combined)

compar[3,] <- combined
```

------------------------------------------------------------------------


\newpage

## Inference/Interpretation


Since we can't plot out a graph for all the dimensions and there's no coefficients, we can't interpret the model besides looking at their evaluation metrics

Remembering the way that KNN works is by making a grid that has a point in each part of the space, and looks at the k nearest points in the space and their corresponding categories, then predicting that point in the space to be the majority category.

If a new observation is taken, then we will find it on the grid and see what the KNN model predicted.

------------------------------------------------------------------------

\newpage

## Optimising F1 Score

```{r}

#| warning: false
#| message: false

library(caret)
library(pROC)
library(class)


# Finding Maximum F1 Score and Corresponding Tau
knn_predict_probabilities <- predict(knn_model, newdata = valid, type = "prob")[, "1"]

F1_score_vec <- vector()
tau <- vector()
seq_i <- seq(min(knn_predict_probabilities) + 0.0001 ,max(knn_predict_probabilities) - 0.0001,length = 1000)
index <- 1

for (i in seq_i) {
  yhat_knn <- ifelse(knn_predict_probabilities >= i,'Revenue', "No Revenue")
  y_knn <- matrix(ifelse(as.numeric(unlist(valid[,16]))==2, 'Revenue', "No Revenue"))
  confmat <- table(yhat_knn, y_knn, dnn = c('Predicted label', 'True label'))

  caretmat <- confusionMatrix(as.factor(yhat_knn), as.factor(y_knn), positive = 'Revenue')
   
  F1_score_vec[index] <- caretmat$byClass["F1"]
  tau[index] <- i
  index <- index + 1
}

df <- data.frame(x = tau, y = F1_score_vec)
mf_knn <- max(F1_score_vec)
max_F1_tau_knn <- df$x[df$y == mf_knn]

# Plotting F Score Account Tau
df <- data.frame(x = tau, y = F1_score_vec)

f1_t_3 <- ggplot() + geom_line(data = df, aes(x = x, y = y)) + geom_vline(xintercept = mean(max_F1_tau_knn), color = "red", linetype = "dashed") + theme_minimal() + labs(x = "Tau", y = "F1 Score", title = "F1 Score Against Different Desician Rule Thresholds (Tau) for a KNN Model")
f1_t_3

# ROC Curve Plot
pred_knn <- prediction(as.numeric(knn_predict_probabilities), as.numeric(valid[,16]))

perf_knn  <- performance(pred_knn, 'tpr', 'fpr')
plot.new()        # Start a new plot (if needed)
plot(perf_knn, colorize = FALSE, col = 'black')
lines(c(0,1), c(0,1), col = 'gray', lty = 4)

yhat_knn <- ifelse(knn_predict_probabilities >= max_F1_tau_knn,'Revenue', "No Revenue")
y_knn <- matrix(ifelse(as.numeric(unlist(valid[,16]))==2, 'Revenue', "No Revenue"))
cat("Confusion Matrix using Predicted Probabilities")
(confmat <- table(yhat_knn, y_knn, dnn = c('Predicted label', 'True label')))

caretmat_knn <- confusionMatrix(as.factor(yhat_knn), as.factor(y_knn), positive = 'Revenue')

TPR_knn <- caretmat_knn$byClass["Sensitivity"]  # True Positive Rate (Recall)
FPR_knn <- 1 - caretmat_knn$byClass["Specificity"]

points(
  x = FPR_knn, 
  y = TPR_knn,
  pch = 19,          # Solid circle point
  col = "red",       # Color of the point
  cex = 1          # Point size
)

text(
  x = FPR_knn + 0.15,
  y = TPR_knn - 0.16,  # Second label: slightly below the point
  labels = paste("F1 Score:", round(mf_knn, 3)),  # Example metric
  col = "black"
)

text(
  x = FPR_knn + 0.15,
  y = TPR_knn - 0.08,  # Second label: slightly below the point
  labels = paste("Threshold:", round(mean(max_F1_tau_knn), 3)),  # Example metric
  col = "black"
)

# Maximum F1 Score And Corresponding Threshold
mean(max_F1_tau_knn)
mf_knn

```
------------------------------------------------------------------------

\newpage

## Prediction

```{r}
```
