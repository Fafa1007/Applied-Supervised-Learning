# K-Nearest Neighbours (KNN)

Apply 10-fold cross-validation throughout to measure classification accuracy

## Modelling

```{r}
#| warning: false

library(caret)
library(class) 

set.seed(4046)

# Firstly, define the range of k values for tuning (1 to 30)
N <- 30

# Secondly, perform k-Nearest Neighbors (kNN) with 10-fold cross-validation to find the optimal number of neighbors (k) for classification
knn_cv <- train(Revenue ~ ., 
                data = train,
                method = "knn",
                trControl = trainControl(method = "cv", number = 10),
                tuneGrid = expand.grid(k = 1:N))

# Lastly, a plot of the performance of each of the neighbours
plot(knn_cv)

save(knn_cv, file = 'R Data/K Nearest Neighbours Model.Rdata')

```

------------------------------------------------------------------------

\newpage

## Model Evaluation 

```{r}
library(MLmetrics)
library(caret)
library(knitr)
library(dplyr)
library(ROCR)

# Firstly, get predicted probabilities or predicted classe (gives the same conclusions) from model
knn_predict_probabilities <- predict(knn_cv, newdata = valid, type = "prob")[, "1"]

# Secondly, create a confusion matrix
yhat <- ifelse(knn_predict_probabilities >= 0.5,'Revenue', "No Revenue")
y <- matrix(ifelse(as.numeric(unlist(valid[,16]))==1, 'Revenue', "No Revenue"))
cat("Confusion Matrix using Predicted Probabilities")
(confmat <- table(yhat, y, dnn = c('Predicted label', 'True label')))

caretmat <- confusionMatrix(as.factor(yhat), as.factor(y), positive = 'Revenue')
caretmat 

Sensitivity <- caretmat$byClass["Sensitivity"]  # True Positive Rate (Recall)
Specificity <- caretmat$byClass["Specificity"]  # True Negative Rate
Precision <- caretmat$byClass["Precision"]      # Positive Predictive Value
F1_score <- caretmat$byClass["F1"]              # F1 Score
Accuracy <- caretmat$overall["Accuracy"]        # Accuracy
Missclassification_Rate <- mean(yhat != y)

# AUC (only works with binary data)
yhat <- ifelse(knn_predict_probabilities >= 0.5, 1, 0)
y <- matrix(ifelse(as.numeric(unlist(valid[,16]))==2, 1, 0))
pred_obj <- prediction(y, yhat)
ROC_AUC <- performance(pred_obj, measure = "auc")@y.values[[1]]

# Lastly, put them all into a table
combined <- cbind(Sensitivity, Specificity, Precision, F1_score,Accuracy, Missclassification_Rate, ROC_AUC)
rownames(combined) <- c("Polynomial Logistic Regression")
kable(combined)

compar[3,] <- combined
```

------------------------------------------------------------------------

\newpage

## Inference/Interpretation

Since we can't plot out a graph for all the dimensions and there's no coefficients, we can't interpret the model besides looking at their evaluation metrics

Remembering the way that KNN works is by making a grid that has a point in each part of the space, and looks at the k nearest points in the space and their corresponding categories, then predicting that point in the space to be the majority category.

If a new observation is taken, then we will find it on the grid and see what the KNN model predicted.

------------------------------------------------------------------------

\newpage

## Optimising F1 Score

```{r}
library(caret)
library(pROC)
library(class)

N <- 30

# Convert your target variable to a factor with valid level names
train$Revenue <- factor(
  train$Revenue,
  levels = c(0, 1),  # Replace with your original values
  labels = c("No", "Yes")  # Use valid R names (no numbers/special characters)
)

valid$Revenue <- factor(
  valid$Revenue,
  levels = c(0, 1),  # Same as above
  labels = c("No", "Yes")  # Must match training set labels
)

# Configure cross-validation with class probabilities
ctrl <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,      # Enable probability estimates
  savePredictions = "final",
  summaryFunction = twoClassSummary
)

# Train KNN model with kknn (supports probabilities)
knn_cv <- train(
  Revenue ~ .,
  data = train,
  method = "kknn",        # Use kknn method instead of knn3
  trControl = ctrl,
  tuneGrid = expand.grid(
    kmax = 1:N,           # Number of neighbors (equivalent to k)
    distance = 2,         # Minkowski power (2 = Euclidean)
    kernel = "optimal"    # Weighting kernel
  ),
  metric = "ROC"
)
########## first way
# Get predicted probabilities for the positive class (e.g., "Yes")
prob_pred_knn <- predict(knn_cv, newdata = valid, type = "prob")[, "Yes"]

# Create ROC curve

pred_knn <- prediction(as.numeric(prob_pred_knn), as.numeric(valid[,16]))
perf_knn  <- performance(pred_knn, 'tpr', 'fpr')
plot.new()        # Start a new plot (if needed)
plot(perf_knn, colorize = FALSE, col = 'black')
lines(c(0,1), c(0,1), col = 'gray', lty = 4)

auc <- performance(pred_knn, measure = 'auc')@y.values[[1]]

########### second way
# Get predicted probabilities for the positive class
prob_pred_knn <- predict(knn_cv, newdata = valid, type = "prob")[, "Yes"]

library(pROC)

# Create ROC object
roc_obj <- roc(
  response = valid$Revenue,  # True labels (factor with levels "No"/"Yes")
  predictor = prob_pred_knn      # Predicted probabilities for "Yes"
)

# Extract TPR and FPR
tpr <- roc_obj$sensitivities
fpr <- 1 - roc_obj$specificities

library(ggplot2)

# Create a data frame of TPR and FPR
roc_data <- data.frame(FPR = fpr, TPR = tpr)

# Plot using ggplot2
ggplot(roc_data, aes(x = FPR, y = TPR)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "ROC Curve for KNN Model",
    x = "False Positive Rate (FPR)",
    y = "True Positive Rate (TPR)"
  ) +
  theme_minimal()
```

------------------------------------------------------------------------

\newpage

## Prediction

```{r}
```
