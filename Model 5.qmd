# Random Forest

Apply 10-fold cross-validation throughout to measure OOB accuracy

## Modelling

Motivating clearly for our selected hyperparameters

```{r}
library(randomForest)

# Firstly, fit a basic Random Forest model with default hyperparameters
rf <- randomForest(Revenue ~ ., data = train,
                           ntree = 1000,
                           na.action = na.exclude)

# Secondly, calculating the missclassification error rate on the validation set
rf_pred   <- predict(rf, newdata = valid)
rf_mis   <- mean(as.numeric(unlist(y_valid)) != as.numeric(rf_pred))
```

Hyperparameter Tuned Random Forest

Hyper parameter tuning on the following: **mtry** Number of variables sampled at each split, **splitrule** Rule used to determine splits at each node, **min.node.size** Minimum number of observations required in a terminal node (leaf)

```{r}
library(randomForest)
library(caret)
library(ranger) # built in parallelisation 
library(dplyr)

# Firstly, define a grid of hyperparameters to search over
rf_grid <- expand.grid(mtry = 2:(ncol(train) - 1),
             splitrule = c('gini', 'hellinger'),
             min.node.size = c(1, 5, 10))

# Secondly, set up control parameters for the training process
ctrl <- trainControl(method = 'oob', verboseIter = F)

# Lastly, perform a grid search to find the best hyperparameters using the ranger package
# This is also our Tuned Random Forest Model
rf_gridsearch <- train(Revenue ~ .,
                   data = train,
                   method = 'ranger',
                   num.trees = 1000,
                   trControl = ctrl,
                   tuneGrid = rf_grid,
                   importance = 'impurity')
plot(rf_gridsearch)
rf_gridsearch$finalModel
rf_gridsearch$bestTune
```

------------------------------------------------------------------------

\newpage

## Inference/Interpretation

------------------------------------------------------------------------

\newpage

## Model Evaluation

```{r}
library(MLmetrics)
library(caret)
library(knitr)
library(dplyr)
library(ROCR)

# Firstly, get predicted probabilities or predicted classe (gives the same conclusions) from the regularised logistic model
pred_class <- predict(rf_gridsearch,x_valid)

# Secondly, create a confusion matrix
yhat <- ifelse(pred_probabilities == 1,'Revenue', "No Revenue")
y <- matrix(ifelse(as.numeric(unlist(y_valid))==2, 'Revenue', "No Revenue"))
cat("Confusion Matrix using Predicted Probabilities")
(confmat <- table(yhat, y, dnn = c('Predicted label', 'True label')))

# Thirdly, get the Accuracy, F1 Score, Precision, Recall, Specificity which we can calculate from the confusion matrix .... or us the caret package 
caretmat <- confusionMatrix(as.factor(yhat), as.factor(y), positive = 'Revenue')
caretmat 

Sensitivity <- caretmat$byClass["Sensitivity"]  # True Positive Rate (Recall)
Specificity <- caretmat$byClass["Specificity"]  # True Negative Rate
Precision <- caretmat$byClass["Precision"]      # Positive Predictive Value
F1_score <- caretmat$byClass["F1"]              # F1 Score
Accuracy <- caretmat$overall["Accuracy"]        # Accuracy
Missclassification_Rate <- mean(yhat != y)

# AUC (only works with binary data)
yhat <- ifelse(pred_probabilities == 1, 1, 0)
y <- matrix(ifelse(as.numeric(unlist(y_valid))==2, 1, 0))
pred_obj <- prediction(y, yhat)
ROC_AUC <- performance(pred_obj, measure = "auc")@y.values[[1]]

# Lastly, put them all into a table
compar <- rbind(Sensitivity, Specificity, Precision, F1_score,Accuracy, Missclassification_Rate, ROC_AUC)
colnames(compar) <- c("Values")
compar %>% kable(caption = "Logistic Regression with Elastic Regularisation Evaluation Metric")
```

------------------------------------------------------------------------

\newpage

## Optimizing F1 Score

------------------------------------------------------------------------

\newpage

## Prediction
