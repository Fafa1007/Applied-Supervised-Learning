# Random Forest

Apply 10-fold cross-validation throughout to measure OOB accuracy

## Modelling

Motivating clearly for our selected hyperparameters

```{r}
library(randomForest)
set.seed(4026)

# Fit a basic Random Forest model with default hyperparameters
rf <- randomForest(Revenue ~ ., data = train,
                           ntree = 1000,
                           na.action = na.exclude)
```

Hyperparameter Tuned Random Forest

Hyper parameter tuning on the following: **mtry** Number of variables sampled at each split, **splitrule** Rule used to determine splits at each node, **min.node.size** Minimum number of observations required in a terminal node (leaf)

```{r}
library(randomForest)
library(caret)
library(ranger) # built in parallelisation 
library(dplyr)
set.seed(4026)

# Firstly, define a grid of hyperparameters to search over
rf_grid <- expand.grid(mtry = 3:(ncol(train) - 1),
             splitrule = c('gini', 'hellinger'),
             min.node.size = c(1, 5, 10))

# Secondly, set up control parameters for the training process using 10 fold CV and OOB
ctrl <- trainControl(method = 'oob', verboseIter = F, number = 10)

# Lastly, perform a grid search to find the best hyperparameters using the ranger package
# This is also our Tuned Random Forest Model
rf_gridsearch <- train(Revenue ~ .,
                   data = train,
                   method = 'ranger',
                   num.trees = 1000,
                   trControl = ctrl,
                   tuneGrid = rf_grid,
                   importance = 'impurity')
plot(rf_gridsearch)
rf_gridsearch$finalModel
rf_gridsearch$bestTune
```

------------------------------------------------------------------------

\newpage

## Inference/Interpretation

**Variable Importance Plot**

```{r}
# Variable Importance Plot 
plot(varImp(rf_gridsearch))
```

1.  **Top Important Features**:

    -   **PageValues** and **ProductRelated_Duration** are the most critical features, directly supporting the study's focus on leveraging high-value page interactions and product-related engagement to predict purchasing intent. The study emphasizes that combining clickstream data (e.g., navigation paths) with session information significantly improves prediction accuracy. Higher `PageValues` indicate a stronger likelihood of revenue generation, while longer `ProductRelated_Duration` reflects deeper engagement with product-related content, both of which are key predictors of purchasing intent.

    -   **ExitRates** also play a vital role, aligning with the study's second module on predicting site abandonment likelihood. Lower exit rates are associated with higher engagement and a reduced likelihood of users leaving the site without converting, which is critical for improving purchase conversion rates.

2.  **Moderately Important Features**:

    -   **ProductRelated** and **Administrative_Duration** highlight the importance of user engagement with product-related pages and administrative tasks, respectively. The study uses aggregated pageview data and session information to predict purchasing intent, and these features contribute to understanding user behavior. Users who visit more product-related pages or spend time on administrative tasks (e.g., account management) are more likely to generate revenue, reinforcing the study's approach of combining multiple data sources for accurate predictions.

    -   **BounceRates** further underscores the study's focus on reducing site abandonment. Lower bounce rates indicate higher engagement, which is essential for improving conversion rates and aligns with the study's goal of identifying visitors at risk of abandoning the site.

3.  **Less Important Features**:

    -   **Informational_Duration** and **Informational** have a smaller impact on revenue generation compared to product-related engagement, reflecting the study's finding that while informational content is valuable, it is less predictive of purchasing intent than clickstream data from navigation paths.

    -   **Month-related Features** (e.g., `MonthFeb`, `MonthDec`) show varying importance, supporting the study's observation of seasonal trends in shopper behavior. While these features provide some predictive power, they are secondary to engagement metrics like `PageValues` and `ProductRelated_Duration`.

    -   **VisitorType** (e.g., Returning Visitor, Other) offers additional context but is less critical than engagement metrics. This aligns with the study's use of session information to refine predictions, where visitor type complements but does not overshadow the importance of behavioral data.

**Partial Dependency Plots**

```{r}
library(gridExtra)
library(grid)
library(pdp)
library(ranger)
library(caret)

predict_function <- function(model, newdata) {
  predict(model, data = newdata)$predictions
}

train_df <- as.data.frame(train)

final_model <- rf_gridsearch$finalModel
p1 <- partial(final_model, "ProductRelated_Duration", train = as.matrix(x_train))

```

------------------------------------------------------------------------

\newpage

## Model Evaluation

Model Evaluation of Tuned Random Forest Model

```{r}
library(MLmetrics)
library(caret)
library(knitr)
library(dplyr)
library(ROCR)

# Firstly, get predicted probabilities or predicted classe (gives the same conclusions) from the model
pred_class <- predict(rf_gridsearch,x_valid)

# Secondly, create a confusion matrix
yhat <- ifelse(pred_class == 1,'Revenue', "No Revenue")
y <- matrix(ifelse(as.numeric(unlist(y_valid))==2, 'Revenue', "No Revenue"))
cat("Confusion Matrix using Predicted Probabilities")
(confmat <- table(yhat, y, dnn = c('Predicted label', 'True label')))

# Thirdly, get the Accuracy, F1 Score, Precision, Recall, Specificity which we can calculate from the confusion matrix .... or us the caret package 
caretmat <- confusionMatrix(as.factor(yhat), as.factor(y), positive = 'Revenue')
caretmat 

Sensitivity <- caretmat$byClass["Sensitivity"]  # True Positive Rate (Recall)
Specificity <- caretmat$byClass["Specificity"]  # True Negative Rate
Precision <- caretmat$byClass["Precision"]      # Positive Predictive Value
F1_score <- caretmat$byClass["F1"]              # F1 Score
Accuracy <- caretmat$overall["Accuracy"]        # Accuracy
Missclassification_Rate <- mean(yhat != y)

# AUC (only works with binary data)
yhat <- ifelse(pred_class == 1, 1, 0)
y <- matrix(ifelse(as.numeric(unlist(y_valid))==2, 1, 0))
pred_obj <- prediction(y, yhat)
ROC_AUC <- performance(pred_obj, measure = "auc")@y.values[[1]]

# Lastly, put them all into a table
combined <- cbind(Sensitivity, Specificity, Precision, F1_score,Accuracy, Missclassification_Rate, ROC_AUC)
rownames(combined) <- c("Tuned Random Forest")
kable(combined)

compar[5,] <- combined
```

------------------------------------------------------------------------

\newpage

## Optimizing F1 Score

------------------------------------------------------------------------

\newpage

## Prediction
