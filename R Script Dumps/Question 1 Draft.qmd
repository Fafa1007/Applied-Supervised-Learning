# Question 1 - Modelling

```{r}
train <- read.csv("online_shopping_train.csv")
test <- read.csv("online_shopping_testing.csv")
valid <- read.csv("online_shopping_valid.csv")
head(train)
```

First convert categorical variables into factor type, stating for which features this was done.

```{r}
factor_cols <- c("Month", "OperatingSystems", "Browser", "VisitorType", "Weekend", "Revenue")

scale_cols <- c("Administrative","Administrative_Duration", "Informational", "Informational_Duration","ProductRelated","SpecialDay", "ProductRelated_Duration","BounceRates", "ExitRates", "PageValues")

train[factor_cols] <- lapply(train[factor_cols], factor)
train[scale_cols] <- lapply(train[scale_cols], scale)
y_train <- train["Revenue"]
x_train <- train[, -16]

valid[factor_cols] <- lapply(valid[factor_cols], factor)
valid[scale_cols] <- lapply(valid[scale_cols], scale)
y_valid <- valid["Revenue"]
x_valid <- valid[,-16]

test[head(factor_cols,-1)] <- lapply(test[head(factor_cols,-1)], factor)
test[scale_cols] <- lapply(test[scale_cols], scale)
```

------------------------------------------------------------------------

# Question 1: Logistic regression with a linear decision boundary.

Apply elastic-net regularization to this model, motivating for the choice of α and λ.

```{r}
library(tidyverse)
library(knitr)
library(broom)

# logistic regression model to the entire datase
log_mod <- glm(Revenue ~ ., family = binomial,data = train)
log_mod %>% 
  tidy() %>%
  kable(digits = 2, caption = 'Saturated logistic regression model fitted to the online shopping dataset')
```

```{r}
# Ridge L2 Regularisation
library(glmnet)
ridge <- glmnet(x_train, as.numeric(unlist(y_train)), alpha = 0, standardize = T,lambda = exp(seq(-1, 4, length.out = 100)))
plot(ridge, xvar = 'lambda', label = T)

# Cross Validation
set.seed(1)
ridge_cv <- cv.glmnet(as.matrix(x_valid), as.numeric(unlist(y_valid)),
            alpha = 0, nfolds = 10, type.measure = 'class', standardize = T, ,
            lambda = exp(seq(-10, 10, length.out = 100)))
plot(ridge_cv)
abline(h = ridge_cv$cvup[which.min(ridge_cv$cvm)], lty = 2)

round(cbind(coef(ridge_cv, s = 'lambda.min'), coef(ridge_cv, s = 'lambda.1se')), 3)
```

```{r}
# Lasso L1 Regularisation 
library(glmnet)
lasso <- glmnet(x_train, as.numeric(unlist(y_train)), alpha = 1, standardize = T)
plot(lasso, xvar = 'lambda', label = T)

# Cross Validation to find best lambda
set.seed(1)
lasso_cv <- cv.glmnet(as.matrix(x_train), as.numeric(unlist(y_train)),
                  alpha = 1, nfolds = 10, type.measure = 'class', standardize = T)
plot(lasso_cv)

round(coef(lasso_cv, s = 'lambda.1se'), 3)
```

```{r}
# Elastic Net
library(glmnet)
library(plotmo)
library(glmnetUtils)

elasticnet <- cva.glmnet(as.numeric(Revenue) ~ ., train, alpha = seq(0, 1, 0.1), nfolds = 10)
plot(elasticnet, main ="Cross Validation Error as lambda increases for 
different fixed values of alpha")
```

```{r}
alphas <- elasticnet$alpha 
cv_mses <- sapply(elasticnet$modlist,
  function(mod) min(mod$cvm) 
  )
mod <- elasticnet$
lowest_mse <- min(cv_mses)
best_alpha <- alphas[which.min(cv_mses)]
best_lambda <- round(elasticnet$modlist[[which.min(cv_mses)]]$lambda.min,5)

print(paste("The lowest CV MSE is", lowest_mse))
print(paste("The Alpha corresponding to this minumum is", best_alpha))
print(paste("The Lamda acorresponding to this minumum is", best_lambda))

plot(alphas, cv_mses, 'b', lwd = 2, pch = 16, col = 'navy', xlab = expression(alpha), ylab = 'CV MSE') #Scale is crucial, this is still very granular!
abline(v = best_alpha, lty = 3, col = 'red')
```

\newpage

------------------------------------------------------------------------

# Question 2: Logistic regression with a non-linear decision boundary.

You may specify the model and regularise (or not) in any way you wish, just be sure to clearly state your final model.

```{r}
# logistic regression model to the entire datase
log_mod <- glm(Revenue ~ ., family = binomial,data = train)
log_mod %>% 
  tidy() %>%
  kable(digits = 2, caption = 'Saturated logistic regression model fitted to the online shopping dataset')

```

```{r}
# Explanatory Data Analysis for Non-linear Relationships
library(ggplot2)
library(patchwork)

# Choose the variables to plot out by checking the significance level of the variables in the glm output
dat <- train
scale_cols <- c("Informational", "ProductRelated_Duration", "ExitRates", "PageValues")

# Generate all pairwise combinations of variables
combinations <- expand.grid(scale_cols, scale_cols, stringsAsFactors = FALSE)

# Filter out combinations where the variables are the same
combinations <- combinations[combinations$Var1 != combinations$Var2, ]

# Create a list to store individual scatterplots
plot_list <- list()

# Loop through each combination and create a scatterplot
for (i in 1:nrow(combinations)) {
  var1 <- combinations$Var1[i]
  var2 <- combinations$Var2[i]
  
  p <- ggplot(dat, aes(x = .data[[var1]], y = .data[[var2]], color = Revenue)) +
    geom_point(alpha = 0.5) +
    labs(x = var1, y = var2) +
    theme_minimal() +
    theme(legend.position = "none")  # Remove legend for individual plots
  plot_list[[paste(var1, var2, sep = "_")]] <- p
}

# Combine the plots into a grid using patchwork
combined_plot <- wrap_plots(plot_list, ncol = 3)  # Arrange in 3 columns
combined_plot
```

```{r}
# Polynomial Logistic Regression
poly_log1 <- glm(Revenue ~ 
            Informational + I(Informational^2) + I(Informational^3) +I(Informational^4) + 
            ProductRelated_Duration + I(ProductRelated_Duration^2) + I(ProductRelated_Duration^3) + I(ProductRelated_Duration^4) +
            ExitRates + I(ExitRates^2) + I(ExitRates^3) + I(ExitRates^4) + 
            PageValues + I(PageValues^2) + I(PageValues^3) + I(PageValues^4) + .
            ,data = train, family = 'binomial')
poly_log1 %>% 
  tidy() %>%
  kable(digits = 2, caption = 'Polynomial logistic regression model fitted to the online shopping dataset using all the varibles')

poly_log2 <- glm(Revenue ~ 
            Informational + I(Informational^2) + I(Informational^3) +I(Informational^4) + 
            ProductRelated_Duration + I(ProductRelated_Duration^2) + I(ProductRelated_Duration^3) + I(ProductRelated_Duration^4) +
            ExitRates + I(ExitRates^2) + I(ExitRates^3) + I(ExitRates^4) + 
            PageValues + I(PageValues^2) + I(PageValues^3) + I(PageValues^4) 
            ,data = train, family = 'binomial')
poly_log2 %>% 
  tidy() %>%
  kable(digits = 2, caption = 'Polynomial logistic regression model fitted to the online shopping dataset using only the polynomial variables')
```

------------------------------------------------------------------------

# Question 4: A classification tree.

Apply appropriate pruning and decide (with motivation) on a tree size.

```{r}
library(tree)
# First, grow a slightly larger tree
shopping_bigtree <- tree(Revenue ~ ., data = train,
                        control = tree.control(nobs = nrow(na.omit(train)),
                                               mindev = 0.005))

# CV to find the best number of tree nodes
set.seed(28)
shopping_cv <- cv.tree(shopping_bigtree, FUN = prune.misclass) 
#Use classification error rate for pruning

# Make the CV plot
plot(shopping_cv$size, shopping_cv$dev, type = 'o',
     pch = 16, col = 'navy', lwd = 2,
     xlab = 'Number of terminal nodes', ylab='CV error')

shopping_cv$k[1] <- 0 #Don't want no -Inf
alpha <- round(shopping_cv$k,1)
axis(3, at = shopping_cv$size, lab = alpha, cex.axis = 0.8)
mtext(expression(alpha), 3, line = 2.5, cex = 1.2)
axis(side = 1, at = 1:max(shopping_cv$size))

T <- shopping_cv$size[which.min(shopping_cv$dev)] #The minimum CV Error
abline(v = T, lty = 2, lwd = 2, col = 'red')

# Plot the pruned Tree
shopping_pruned <- prune.misclass(shopping_bigtree, best = T)
plot(shopping_pruned)
text(shopping_pruned, pretty = 0)
```

------------------------------------------------------------------------

# Question 5:

Motivating clearly for your selected hyperparameters

```{r}
library(randomForest)
shopping_rf <- randomForest(Revenue ~ ., data = train,
                           ntree = 250,
                           importance = T,
                           na.action = na.exclude)
shopping_rf
```
