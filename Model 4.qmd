# Classification Tree

Apply 10-fold cross-validation throughout to measure classification accuracy

## Modelling

Finding and motivating our choice of the tree size we wish to prune our over fitted tree towards.

```{r}
library(tree)
# First, grow an overfitted tree
tree_overfit <- tree(Revenue ~ ., data = train,
                        control = tree.control(nobs = nrow(na.omit(train)),
                                               mindev = 0.005))

# Second, 10 fold CV to find the best number of tree nodes to prune
tree_cv <- cv.tree(tree_overfit, FUN = prune.misclass, K = 10) 
#Use classification error rate for pruning
```

```{r}
# Thirdly, make the CV plot to visually observe the best number of tree nodes to prune with the lowest cross validation error
plot(tree_cv$size, tree_cv$dev, type = 'o',
     pch = 16, col = 'navy', lwd = 2,
     xlab = 'Number of terminal nodes', ylab='CV error')

tree_cv$k[1] <- 0 #Don't want no -Inf
alpha <- round(tree_cv$k,1)
axis(3, at = tree_cv$size, lab = alpha, cex.axis = 0.8)
mtext(expression(alpha), 3, line = 2.5, cex = 1.2)
axis(side = 1, at = 1:max(tree_cv$size))

# T is the hyper parameter for the number of terminal nodes
T <- tree_cv$size[which.min(tree_cv$dev)] #The minimum CV Error
abline(v = T, lty = 2, lwd = 2, col = 'red')
```

Pruned Tree Model

```{r}
# The pruned Tree Model
pruned_tree <- prune.misclass(tree_overfit, best = T)
```

------------------------------------------------------------------------

\newpage

## Inference/Interpretation

1.  **Primary Split by PageValues and Month**:

    -   The tree's root node splits the dataset into two main branches based on the `PageValues` feature. If `PageValues` is less than -0.262927 (63.76% of the data), the tree further splits based on the `Month` feature. For months like February, March, and May, the model predicts **no revenue (class 0)** with 100% confidence.

    -   For other months, the prediction depends on additional splits, such as `ProductRelated_Duration`. This suggests that `PageValues` and `Month` are critical features for distinguishing between revenue-generating and non-revenue-generating visits, with certain months strongly indicating no revenue.

2.  **Impact of BounceRates and PageValues on Revenue Generation**:

    -   When `PageValues` exceeds -0.262927 (21.24% of the data), the tree evaluates `BounceRates`. If `BounceRates` is below -0.451897, the model predicts **revenue generation (class 1)** with high confidence (74.12%).

    -   Further splits based on `PageValues` refine this prediction, with higher `PageValues` (\> 0.825014) increasing the probability of revenue generation to 85%. This indicates that low `BounceRates` and high `PageValues` are strong indicators of revenue generation.

3.  **Interaction of ProductRelated_Duration and Month**:

    -   In the branch where `BounceRates` exceeds -0.451897, the tree uses `Month` and `ProductRelated_Duration` to make predictions. For months like August, December, July, June, May, October, and September, the model predicts **no revenue (class 0)** if `ProductRelated_Duration` is above -0.0655534 (75.19% confidence).

    -   However, for February, March, and November, the model leans toward **revenue generation (class 1)** (56.99% confidence).

```{r}
# Pruned Classification Tree
plot(pruned_tree)
text(pruned_tree, pretty = 0)
pruned_tree
```

------------------------------------------------------------------------

\newpage

## Model Evaluation

```{r}
library(MLmetrics)
library(caret)
library(knitr)
library(dplyr)
library(ROCR)

# Firstly, get predicted probabilities or predicted classe (gives the same conclusions) from the regularised logistic model
pred_class <- predict(pruned_tree,x_valid, type = "class")

# Secondly, create a confusion matrix
yhat <- ifelse(pred_class == 1,'Revenue', "No Revenue")
y <- matrix(ifelse(as.numeric(unlist(y_valid))==2, 'Revenue', "No Revenue"))
cat("Confusion Matrix using Predicted Probabilities")
(confmat <- table(yhat, y, dnn = c('Predicted label', 'True label')))

# Thirdly, get the Accuracy, F1 Score, Precision, Recall, Specificity which we can calculate from the confusion matrix .... or us the caret package 
caretmat <- confusionMatrix(as.factor(yhat), as.factor(y), positive = 'Revenue')
caretmat 

Sensitivity <- caretmat$byClass["Sensitivity"]  # True Positive Rate (Recall)
Specificity <- caretmat$byClass["Specificity"]  # True Negative Rate
Precision <- caretmat$byClass["Precision"]      # Positive Predictive Value
F1_score <- caretmat$byClass["F1"]              # F1 Score
Accuracy <- caretmat$overall["Accuracy"]        # Accuracy
Missclassification_Rate <- mean(yhat != y)

# AUC (only works with binary data)
yhat <- ifelse(pred_class == 1, 1, 0)
y <- matrix(ifelse(as.numeric(unlist(y_valid))==2, 1, 0))
pred_obj <- prediction(y, yhat)
ROC_AUC <- performance(pred_obj, measure = "auc")@y.values[[1]]

# Lastly, put them all into a table
combined <- cbind(Sensitivity, Specificity, Precision, F1_score,Accuracy, Missclassification_Rate, ROC_AUC)
rownames(combined) <- c("Classification Tree")
kable(combined)

compar <- rbind(compar, combined)
```

------------------------------------------------------------------------

\newpage

## Optimizing F1 Score

------------------------------------------------------------------------

\newpage

## Prediction
