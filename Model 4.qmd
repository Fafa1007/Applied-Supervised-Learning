# Classification Tree

## Modelling

Apply appropriate pruning and decide (with motivation) on a tree size.

```{r}
library(tree)
# First, grow a slightly larger tree
shopping_bigtree <- tree(Revenue ~ ., data = train,
                        control = tree.control(nobs = nrow(na.omit(train)),
                                               mindev = 0.005))

# CV to find the best number of tree nodes
shopping_cv <- cv.tree(shopping_bigtree, FUN = prune.misclass) 
#Use classification error rate for pruning

# Make the CV plot
plot(shopping_cv$size, shopping_cv$dev, type = 'o',
     pch = 16, col = 'navy', lwd = 2,
     xlab = 'Number of terminal nodes', ylab='CV error')

shopping_cv$k[1] <- 0 #Don't want no -Inf
alpha <- round(shopping_cv$k,1)
axis(3, at = shopping_cv$size, lab = alpha, cex.axis = 0.8)
mtext(expression(alpha), 3, line = 2.5, cex = 1.2)
axis(side = 1, at = 1:max(shopping_cv$size))

T <- shopping_cv$size[which.min(shopping_cv$dev)] #The minimum CV Error
abline(v = tree_size, lty = 2, lwd = 2, col = 'red')
```

```{r}
# The pruned Tree Model
shopping_pruned <- prune.misclass(shopping_bigtree, best = T)
plot(shopping_pruned)
text(shopping_pruned, pretty = 0)

```

```{r}
# Predictions
shopping_tree_pred  <- predict(shopping_pruned, newdata = valid)
shopping_tree_mis <- mean(as.numeric(unlist(y_valid)) != as.numeric(shopping_tree_pred))
```

------------------------------------------------------------------------

\newpage

## Inference/Interpretation

------------------------------------------------------------------------

\newpage

## Optimizing F1 Score 

------------------------------------------------------------------------

\newpage

## Prediction