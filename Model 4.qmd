# Classification Tree

## Modelling

Finding and motivating our choice of the tree size we wish to prune our over fitted tree towards.

```{r}
library(tree)
# First, grow an overfitted tree
tree_overfit <- tree(Revenue ~ ., data = train,
                        control = tree.control(nobs = nrow(na.omit(train)),
                                               mindev = 0.005))

# Second, CV to find the best number of tree nodes to prune
tree_cv <- cv.tree(tree_overfit, FUN = prune.misclass) 
#Use classification error rate for pruning
```

```{r}
# Thirdly, make the CV plot to visually observe the best number of tree nodes to prune with the lowest cross validation error
plot(tree_cv$size, tree_cv$dev, type = 'o',
     pch = 16, col = 'navy', lwd = 2,
     xlab = 'Number of terminal nodes', ylab='CV error')

tree_cv$k[1] <- 0 #Don't want no -Inf
alpha <- round(tree_cv$k,1)
axis(3, at = tree_cv$size, lab = alpha, cex.axis = 0.8)
mtext(expression(alpha), 3, line = 2.5, cex = 1.2)
axis(side = 1, at = 1:max(tree_cv$size))

T <- tree_cv$size[which.min(tree_cv$dev)] #The minimum CV Error
abline(v = tree_size, lty = 2, lwd = 2, col = 'red')
```

Pruned Tree Model

```{r}
# The pruned Tree Model
shopping_pruned <- prune.misclass(shopping_bigtree, best = T)
plot(shopping_pruned)
text(shopping_pruned, pretty = 0)

```

------------------------------------------------------------------------

\newpage

## Inference/Interpretation

------------------------------------------------------------------------

\newpage

## Model Evaluation

```{r}
# Predictions
shopping_tree_pred  <- predict(shopping_pruned, newdata = valid)
shopping_tree_mis <- mean(as.numeric(unlist(y_valid)) != as.numeric(shopping_tree_pred))
```

------------------------------------------------------------------------

\newpage

## Optimizing F1 Score

------------------------------------------------------------------------

\newpage

## Prediction
